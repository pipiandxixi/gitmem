---
created_at: '2026-01-08T16:37:32.836932'
date: '2026-01-08'
tags:
- context/project/git_diff
- context/project/pyproject.toml
- context
- project
- diff
- context/user/GEMINI.md
- context/project/README.md
- auto-capture
- user
- context/project/GEMINI.md
- context/project/DESIGN.md
updated_at: '2026-01-08T16:42:41.792586'
---

## context/project/GEMINI.md (16:37:32)
# GitMem Project Context

## Vision
GitMem 是一个利用 Git 作为后端存储的语义化记忆服务。它通过 MCP (Model Context Protocol) 协议为 AI Agent 提供持久化、可回溯、具备“摄入-消化”闭环能力的长期记忆体。

## Core Principles
1. **摄入重于分类**: 原始记忆以“流水账”形式按日归档（`logs/YYYY-MM-DD.md`），保证上下文完整性。
2. **GitOps 驱动**: 利用 Git Commit 作为事件总线，实现无状态的后台处理。
3. **混合架构 (Hybrid)**: 本地 MCP Server (FastMCP) 负责极速读写，远程 Backend (FastAPI) 负责异步 LLM 归纳。
4. **双层存储**: `logs/` 存放原始数据，`knowledge/` 存放结构化知识。

## Current Status
- [x] **混合架构落地**: 
    - `src/server.py`: 本地客户端，支持 Git 缓存读写 + HTTP Trigger。
    - `src/backend.py`: 远程服务端，FastAPI + Async Queue，支持多租户归纳。
- [x] **存储引擎重构**: 
    - 原始记忆存入 `logs/` 子目录。
    - 每日日志头部自动维护累积 Tags。
- [x] **智能归纳服务 (Digest Worker)**:
    - 实现了基于 Git Diff 的增量解析。
    - 实现了 LLM 驱动的话题路由 (`index.yaml`) 和全量重写归纳。
- [x] **多租户支持**: 通过 Trigger 动态拉取用户 Repo，服务无状态化。

## Key Decisions
- **通信协议**: 本地与后台之间采用 RESTful (`POST /trigger-digest`) 而非 SSE，简化架构。
- **状态追踪**: 严格使用 Git Commit Message 中的 `Ref-Commit` 标记，拒绝本地状态文件。
- **归纳策略**: 采用“全量重写 (Full Rewrite)”模式更新知识库文件，确保文档结构的一致性。

## TODOs
- [ ] **语义化检索重构**:
    - **Smart Recall**: 优先检索 `knowledge/` 中的结构化数据，辅以 `logs/` 的最新增量。
    - **History**: 基于 `knowledge` 文件的版本历史生成话题演进图。
    - **Graph**: 生成基于 `index.yaml` 和引用关系的知识热力图。
- [ ] **高级归纳能力**:
    - 支持冲突检测与人工介入标记。
    - 定期全量 Compaction（合并碎片化的小话题）。

## context/project/README.md (16:37:35)
# GitMem

GitMem is a semantic memory service for AI Agents, backed by Git. It treats memory as a "Living Codebase".

## Architecture

GitMem employs a **Hybrid Architecture** to balance latency and intelligence:

1.  **Local MCP Server (`src/server.py`)**: 
    - Runs locally (Stdio) with your Agent (Claude, Windsurf, Gemini).
    - Provides instant `remember` and `recall` capabilities using a local Git cache.
    - Triggers the backend service upon new writes.

2.  **Backend Service (`src/backend.py`)**:
    - Runs permanently (e.g., on Render).
    - Receives `trigger-digest` signals.
    - Runs an asynchronous **Digest Worker** that uses LLMs to summarize raw logs into structured knowledge.

3.  **Storage (`Git Repo`)**:
    - `logs/`: Raw, append-only daily logs (High Fidelity).
    - `knowledge/`: AI-curated, structured markdown files (High Utility).

## Setup

### 1. Requirements
- Python 3.10+
- OpenAI API Key (for digestion)
- A Git Repository (GitHub/GitLab)

### 2. Installation
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 3. Running the Backend
Create `src/digest/.env` with your LLM keys (see `.env.example`).
```bash
python src/backend.py
```

### 4. Configuring Your Agent
Add the local server to your `claude_desktop_config.json` or equivalent:
```json
"gitmem": {
  "command": "python",
  "args": ["/path/to/gitmem/src/server.py"],
  "env": {
    "GITMEM_URL": "https://github.com/your/memory-repo.git",
    "GITMEM_TOKEN": "your-git-token",
    "GITMEM_BACKEND_URL": "http://localhost:8000"
  }
}
```

## Usage
Simply tell your Agent:
> "Remember that the deployment port for QuantServer is 9090."

GitMem will:
1.  Save it to `logs/YYYY-MM-DD.md`.
2.  Trigger the backend.
3.  Update `knowledge/quant_server.md` automatically.

## context/project/DESIGN.md (16:37:38)
# GitMem Technical Design

## 1. 存储架构 (Storage Schema)

### 1.1 目录结构
我们采用 **"双层存储架构"**，区分原始摄入与加工知识。
```text
storage/
├── logs/                 # [User Input] 原始记忆 (Raw Data)
│   ├── YYYY-MM-DD.md     # 按日归档的流水账
│   └── ...
├── knowledge/            # [System Output] 归纳后的知识库 (Digested Data)
│   ├── [topic].md        # 特定话题的结构化知识
│   └── ...
```

### 1.2 Markdown 格式规范 (Raw Logs)
每日日志文件 (`logs/YYYY-MM-DD.md`) 采用追加模式，通过二级标题区分条目。
```markdown
---
date: 2026-01-07
tags: [cumulative, tags, list]
---

## topic_name (10:00:00)
... 记忆正文 ...

## another_topic (11:30:00)
... 更多内容 ...
```

## 2. 核心组件 (Core Components)

### 2.1 MemoryStore (`src/store.py`)
- **Git Wrapper**: 负责 `git init`, `add`, `commit`, `log` 等操作。
- **Metadata Manager**: 负责 Front Matter 的解析与合并。
- **Daily Logger**: 负责将所有 `remember` 请求追加到当日的日志文件中，并更新文件头的累积标签。

### 2.2 Digest Worker (`src/digest/`) - *Planned*
一个独立的后台服务，负责将 `logs/` 中的原始数据归纳为 `knowledge/` 中的结构化知识。
- **架构**: "GitOps for Memory"。
- **状态管理**: 无状态设计。通过 Git Commit Message 中的 `Ref-Commit: <hash>` 标记来追踪处理进度。
- **流程**: 
    1. `git log` 查找上次机器处理的 Commit Hash。
    2. `git diff` 找出自上次处理以来 `logs/` 目录的新增内容。
    3. 调用 LLM 进行语义归纳与合并。
    4. 提交更新到 `knowledge/`，并在 Commit Message 中标记当前的 HEAD Hash。
- **详见**: `src/digest/README.md`

### 2.3 MCP Server (`src/server.py`)
利用 FastMCP 暴露以下工具：
- `remember(content, topic, tags)`: 将记忆追加到 `logs/YYYY-MM-DD.md`。
- `recall(query, topic)`: (短期) 扫描所有日志文件；(长期) 优先查询 `knowledge/`，辅以 `logs/` 的最新增量。
- `history/graph`: *待重构，将基于 knowledge 目录实现。*

## 3. 演进逻辑
- **摄入策略**: 极简追加。用户无需关心文件名，所有上下文自动落盘到当日日志。
- **消化策略**: 异步增量。后台 Worker 负责“整理房间”，将散乱的日志整理为有序的知识。

## context/project/pyproject.toml (16:37:41)
[project]
name = "gitmem"
version = "0.1.0"
description = "Git-backed, Metadata-enriched Memory Service for MCP"
requires-python = ">=3.10"
dependencies = [
    "mcp[cli]>=0.1.0",
    "gitpython>=3.1.0",
    "python-frontmatter>=1.0.0",
    "pyyaml>=6.0",
    "fastapi>=0.100.0",
    "uvicorn>=0.20.0",
    "httpx>=0.24.0",
    "openai>=1.0.0",
    "python-dotenv>=1.0.0",
    "pydantic>=2.0.0"
]

[project.scripts]
gitmem = "src.server:main"

## context/user/GEMINI.md (16:37:44)
## Gemini Added Memories
- For QuantServer project, always follow these core principles: 1) NO hidden defaults for quant variables (must be explicit/from config); 2) Transparent error reporting (no masking errors with fake 100% lines); 3) Strict data integrity validation before calculation; 4) Absolute logic parity between backtest and production modules.
- MANDATORY: All UI/Frontend changes in QuantServer must be verified locally before pushing to Render. Workflow: 1) Run local server via run_server.py; 2) Use browser tools to visit http://localhost:8000/view/QQQ/1y; 3) Check console for Syntax/Type errors; 4) Verify chart rendering and interactions. Do NOT push if local verification fails.

## context/project/git_diff (16:37:46)
diff --git a/backend.log b/backend.log
index 423bd51..cd48332 100644
--- a/backend.log
+++ b/backend.log
@@ -1,5 +1,5 @@
-INFO:     Started server process [8580]
+INFO:     Started server process [35868]
 INFO:     Waiting for application startup.
-2026-01-07 21:50:19,300 - gitmem-backend - INFO - Digest consumer started.
+2026-01-08 16:33:37,751 - gitmem-backend - INFO - Digest consumer started.
 INFO:     Application startup complete.
 INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
diff --git a/src/backend.py b/src/backend.py
index 2819525..b592aa7 100644
--- a/src/backend.py
+++ b/src/backend.py
@@ -2,25 +2,25 @@ import os
 import sys
 import asyncio
 import logging
+import tempfile
+import hashlib
+import shutil
 from contextlib import asynccontextmanager
-from fastapi import FastAPI, HTTPException, BackgroundTasks
+from fastapi import FastAPI, HTTPException
 from pydantic import BaseModel
+from git import Repo
 
 # Add project root to sys.path
 project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
 if project_root not in sys.path:
     sys.path.insert(0, project_root)
 
-from src.store import MemoryStore
 from src.digest.worker import DigestWorker
 
 # Setup Logging
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger("gitmem-backend")
 
-# Initialize Store (for repo path calculation)
-store = MemoryStore()
-
 # Global Queue
 digest_queue = asyncio.Queue()
 
@@ -29,6 +29,56 @@ class TriggerRequest(BaseModel):
     git_url: str
     git_token: str | None = None
 
+# --- Helper ---
+def _prepare_backend_repo(git_url: str, git_token: str | None) -> str:
+    """
+    Prepare a temporary clone of the repo in the system temp directory.
+    This ensures isolation from any local user config or other services.
+    """
+    # 1. Calculate Temp Path
+    url_hash = hashlib.sha256(git_url.encode()).hexdigest()[:12]
+    base_tmp = os.path.join(tempfile.gettempdir(), "gitmem-backend")
+    repo_path = os.path.join(base_tmp, url_hash)
+    
+    os.makedirs(base_tmp, exist_ok=True)
+    
+    # 2. Authenticate URL
+    auth_url = git_url
+    if git_token and git_url.startswith("https://"):
+        auth_url = git_url.replace("https://", f"https://x-access-token:{git_token}@")
+
+    # 3. Clone or Update
+    if not os.path.exists(repo_path):
+        logger.info(f"Cloning to temp: {repo_path}")
+        try:
+            Repo.clone_from(auth_url, repo_path)
+        except Exception as e:
+            logger.error(f"Clone failed: {e}")
+            raise e
+    else:
+        logger.info(f"Updating temp repo: {repo_path}")
+        try:
+            repo = Repo(repo_path)
+            # Update remote in case token changed
+            if 'origin' in repo.remotes:
+                repo.remotes.origin.set_url(auth_url)
+            repo.remotes.origin.pull()
+        except Exception as e:
+            logger.error(f"Pull failed: {e}")
+            # If repo is corrupt, nuking it is a valid backend strategy
+            logger.warning("Nuking corrupt repo and re-cloning...")
+            shutil.rmtree(repo_path)
+            Repo.clone_from(auth_url, repo_path)
+            
+    # Configure Bot Identity for this temp repo
+    repo = Repo(repo_path)
+    with repo.config_writer() as git_config:
+        if not git_config.has_option('user', 'name'):
+            git_config.set_value('user', 'name', "GitMem Backend")
+            git_config.set_value('user', 'email', "backend@gitmem.cloud")
+
+    return repo_path
+
 # --- Background Consumer ---
 async def digest_consumer():
     """Consume tasks from the queue and run the worker."""
@@ -36,14 +86,21 @@ async def digest_consumer():
     while True:
         try:
             task = await digest_queue.get()
-            repo_path = task.get("repo_path")
+            git_url = task.get("git_url")
+            git_token = task.get("git_token")
             
-            if repo_path:
-                logger.info(f"Starting digest for: {repo_path}")
-                # Blocking IO runs in thread pool
-                worker = DigestWorker(repo_path)
-                await asyncio.to_thread(worker.run)
-                logger.info(f"Finished digest for: {repo_path}")
+            if git_url:
+                logger.info(f"Processing digest for: {git_url}")
+                try:
+                    # Prepare repo in thread to avoid blocking loop
+                    repo_path = await asyncio.to_thread(_prepare_backend_repo, git_url, git_token)
+                    
+                    # Run Worker
+                    worker = DigestWorker(repo_path)
+                    await asyncio.to_thread(worker.run)
+                    logger.info(f"Finished digest for: {git_url}")
+                except Exception as e:
+                    logger.error(f"Digest failed for {git_url}: {e}")
             
             digest_queue.task_done()
         except asyncio.CancelledError:
@@ -51,7 +108,7 @@ async def digest_consumer():
         except Exception as e:
             logger.error(f"Error in consumer: {e}")
             # Avoid queue lockup
-            if task: 
+            if 'task' in locals() and task: 
                 digest_queue.task_done()
 
 # --- Lifecycle ---
@@ -74,13 +131,12 @@ app = FastAPI(title="GitMem Backend", lifespan=lifespan)
 async def trigger_digest(request: TriggerRequest):
     """Trigger a background digest for the given repo."""
     try:
-        # Calculate local path using Store logic
-        repo_path = store._get_user_repo_path(request.git_url)
-        
-        # Enqueue
-        await digest_queue.put({"repo_path": repo_path})
-        
-        return {"status": "queued", "repo_path": repo_path}
+        # Just enqueue parameters. Let the worker handle path and cloning.
+        await digest_queue.put({
+            "git_url": request.git_url, 
+            "git_token": request.git_token
+        })
+        return {"status": "queued", "url": request.git_url}
     except Exception as e:
         logger.error(f"Trigger failed: {e}")
         raise HTTPException(status_code=500, detail=str(e))
diff --git a/src/digest/__pycache__/worker.cpython-313.pyc b/src/digest/__pycache__/worker.cpython-313.pyc
index 3558a4a..6fec236 100644
Binary files a/src/digest/__pycache__/worker.cpython-313.pyc and b/src/digest/__pycache__/worker.cpython-313.pyc differ
diff --git a/src/digest/worker.py b/src/digest/worker.py
index 2f5fc74..1f117cf 100644
--- a/src/digest/worker.py
+++ b/src/digest/worker.py
@@ -192,6 +192,14 @@ class GitClient:
         self.repo.index.commit(f"{message}\n\nRef-Commit: {ref_hash}")
         logger.info(f"Committed digest work. Ref: {ref_hash}")
 
+    def push(self):
+        try:
+            origin = self.repo.remote(name='origin')
+            origin.push()
+            logger.info("Pushed changes to remote.")
+        except Exception as e:
+            logger.error(f"Push failed: {e}")
+
 class DigestWorker:
     def __init__(self, repo_path: str):
         self.repo_path = repo_path
@@ -212,14 +220,17 @@ class DigestWorker:
         
         last_hash = self.git.get_last_processed_hash()
         current_head = self.git.repo.head.commit.hexsha
+        logger.info(f"Last Hash: {last_hash}, Current Head: {current_head}")
         
         if last_hash == current_head:
             logger.info("Up to date.")
             return
 
         commits = self.git.get_pending_commits(last_hash)
+        logger.info(f"Found {len(commits)} pending commits.")
         if not commits:
             self.git.commit_work(current_head, "Digest: fast-forward")
+            self.git.push()
             return
         
         # 1. Extract Raw Entries
@@ -228,6 +239,7 @@ class DigestWorker:
         
         if not entries:
              self.git.commit_work(current_head, "Digest: no parseable content")
+             self.git.push()
              return
 
         # 2. Group by Canonical Topic
@@ -259,8 +271,9 @@ class DigestWorker:
             with open(topic_file, 'w') as f:
                 f.write(final_text)
                 
-        # 4. Commit
+        # 4. Commit & Push
         self.git.commit_work(current_head)
+        self.git.push()
 
 import argparse
 import sys
@@ -309,15 +322,6 @@ if __name__ == "__main__":
         # 2. Run Digest
         worker = DigestWorker(repo_path)
         worker.run()
-        
-        # 3. Push changes back
-        if repo.is_dirty() or repo.index.diff(repo.head.commit):
-             # Just in case worker.run() didn't push (it currently only commits)
-             # But our commit_work commits to index.
-             # Ideally worker should push. Let's add push here for safety.
-             origin = repo.remote(name='origin')
-             origin.push()
-             logger.info("Pushed changes to remote.")
              
     except Exception as e:
         logger.error(f"Worker failed: {e}")
diff --git a/src/server.py b/src/server.py
index d627972..b885a08 100644
--- a/src/server.py
+++ b/src/server.py
@@ -10,6 +10,7 @@ if project_root not in sys.path:
 from mcp.server.fastmcp import FastMCP
 from src.store import MemoryStore
 from typing import List, Optional
+from datetime import datetime
 
 # Setup error logging for debugging startup
 LOG_FILE = os.path.expanduser("~/.gitmem/error.log")
@@ -128,27 +129,36 @@ def remember(content: str, topic: str = "global", tags: List[str] = None,
         git_url: The HTTPS URL of your git repository (required for multi-user).
         git_token: Personal Access Token for authentication.
     """
-    result = store.remember(content, topic, tags, dependencies, git_url, git_token)
+    # 1. Strict Configuration Validation
+    # We expect GITMEM_URL to be set in the environment (e.g. Claude Desktop config),
+    # but allow overriding via arguments.
+    current_git_url = git_url or os.environ.get("GITMEM_URL")
+    current_git_token = git_token or os.environ.get("GITMEM_TOKEN")
+
+    if not current_git_url:
+        return "Error: GITMEM_URL is missing. Please configure it in your environment or pass 'git_url' explicitly."
+
+    # 2. Store Memory
+    # We pass the resolved URL/Token explicitly to avoid double-checking in store
+    result = store.remember(content, topic, tags, dependencies, current_git_url, current_git_token)
     
-    # Trigger Remote Backend Digestion (Hybrid Architecture)
-    # Only if storage was successful and we have a backend URL
-    if "Error:" not in result and git_url:
+    # 3. Trigger Remote Backend Digestion
+    # Only if storage was successful.
+    if "Error:" not in result:
         backend_url = os.environ.get("GITMEM_BACKEND_URL", "http://localhost:8000")
         try:
-            # We use a short timeout because we don't want to block the CLI user
-            # waiting for the backend to acknowledge.
-            # Ideally this should be a background task, but in FastMCP sync tool context,
-            # a quick HTTP call is acceptable.
             with httpx.Client(timeout=1.0) as client:
-                payload = {"git_url": git_url, "git_token": git_token}
-                # Fire and forget-ish (we wait for response but don't parse it deeply)
+                payload = {"git_url": current_git_url, "git_token": current_git_token}
                 resp = client.post(f"{backend_url}/trigger-digest", json=payload)
-                if resp.status_code == 200:
-                    pass # Success
-        except Exception:
-            # Silently fail network trigger to avoid annoying the user.
-            # The memory is safely stored in Git anyway.
-            pass
+                
+                if resp.status_code != 200:
+                    # Log failure for debugging
+                    with open(os.path.expanduser("~/.gitmem/trigger_error.log"), "a") as f:
+                        f.write(f"[{datetime.now()}] Backend returned {resp.status_code}: {resp.text}\n")
+        except Exception as e:
+            # Log connection errors but don't fail the user request
+            with open(os.path.expanduser("~/.gitmem/trigger_error.log"), "a") as f:
+                f.write(f"[{datetime.now()}] Trigger failed: {str(e)}\n")
 
     return result
 
diff --git a/sse_server.log b/sse_server.log
deleted file mode 100644
index d45b554..0000000
--- a/sse_server.log
+++ /dev/null
@@ -1,4 +0,0 @@
-Traceback (most recent call last):
-  File "/Users/zhoufan/Public/workspace/gitmem/src/sse_app.py", line 12, in <module>
-    from src.store import MemoryStore
-ModuleNotFoundError: No module named 'src'

## context/project/GEMINI.md (16:39:15)
# GitMem Project Context

## Vision
GitMem 是一个利用 Git 作为后端存储的语义化记忆服务。它通过 MCP (Model Context Protocol) 协议为 AI Agent 提供持久化、可回溯、具备“摄入-消化”闭环能力的长期记忆体。

## Core Principles
1. **摄入重于分类**: 原始记忆以“流水账”形式按日归档（`logs/YYYY-MM-DD.md`），保证上下文完整性。
2. **GitOps 驱动**: 利用 Git Commit 作为事件总线，实现无状态的后台处理。
3. **混合架构 (Hybrid)**: 本地 MCP Server (FastMCP) 负责极速读写，远程 Backend (FastAPI) 负责异步 LLM 归纳。
4. **双层存储**: `logs/` 存放原始数据，`knowledge/` 存放结构化知识。

## Current Status
- [x] **混合架构落地**: 
    - `src/server.py`: 本地客户端，支持 Git 缓存读写 + HTTP Trigger。
    - `src/backend.py`: 远程服务端，FastAPI + Async Queue，支持多租户归纳。
- [x] **存储引擎重构**: 
    - 原始记忆存入 `logs/` 子目录。
    - 每日日志头部自动维护累积 Tags。
- [x] **智能归纳服务 (Digest Worker)**:
    - 实现了基于 Git Diff 的增量解析。
    - 实现了 LLM 驱动的话题路由 (`index.yaml`) 和全量重写归纳。
- [x] **多租户支持**: 通过 Trigger 动态拉取用户 Repo，服务无状态化。

## Key Decisions
- **通信协议**: 本地与后台之间采用 RESTful (`POST /trigger-digest`) 而非 SSE，简化架构。
- **状态追踪**: 严格使用 Git Commit Message 中的 `Ref-Commit` 标记，拒绝本地状态文件。
- **归纳策略**: 采用“全量重写 (Full Rewrite)”模式更新知识库文件，确保文档结构的一致性。

## TODOs
- [ ] **语义化检索重构**:
    - **Smart Recall**: 优先检索 `knowledge/` 中的结构化数据，辅以 `logs/` 的最新增量。
    - **History**: 基于 `knowledge` 文件的版本历史生成话题演进图。
    - **Graph**: 生成基于 `index.yaml` 和引用关系的知识热力图。
- [ ] **高级归纳能力**:
    - 支持冲突检测与人工介入标记。
    - 定期全量 Compaction（合并碎片化的小话题）。

## context/project/README.md (16:39:18)
# GitMem

GitMem is a semantic memory service for AI Agents, backed by Git. It treats memory as a "Living Codebase".

## Architecture

GitMem employs a **Hybrid Architecture** to balance latency and intelligence:

1.  **Local MCP Server (`src/server.py`)**: 
    - Runs locally (Stdio) with your Agent (Claude, Windsurf, Gemini).
    - Provides instant `remember` and `recall` capabilities using a local Git cache.
    - Triggers the backend service upon new writes.

2.  **Backend Service (`src/backend.py`)**:
    - Runs permanently (e.g., on Render).
    - Receives `trigger-digest` signals.
    - Runs an asynchronous **Digest Worker** that uses LLMs to summarize raw logs into structured knowledge.

3.  **Storage (`Git Repo`)**:
    - `logs/`: Raw, append-only daily logs (High Fidelity).
    - `knowledge/`: AI-curated, structured markdown files (High Utility).

## Setup

### 1. Requirements
- Python 3.10+
- OpenAI API Key (for digestion)
- A Git Repository (GitHub/GitLab)

### 2. Installation
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 3. Running the Backend
Create `src/digest/.env` with your LLM keys (see `.env.example`).
```bash
python src/backend.py
```

### 4. Configuring Your Agent
Add the local server to your `claude_desktop_config.json` or equivalent:
```json
"gitmem": {
  "command": "python",
  "args": ["/path/to/gitmem/src/server.py"],
  "env": {
    "GITMEM_URL": "https://github.com/your/memory-repo.git",
    "GITMEM_TOKEN": "your-git-token",
    "GITMEM_BACKEND_URL": "http://localhost:8000"
  }
}
```

## Usage
Simply tell your Agent:
> "Remember that the deployment port for QuantServer is 9090."

GitMem will:
1.  Save it to `logs/YYYY-MM-DD.md`.
2.  Trigger the backend.
3.  Update `knowledge/quant_server.md` automatically.

## context/project/DESIGN.md (16:39:21)
# GitMem Technical Design

## 1. 存储架构 (Storage Schema)

### 1.1 目录结构
我们采用 **"双层存储架构"**，区分原始摄入与加工知识。
```text
storage/
├── logs/                 # [User Input] 原始记忆 (Raw Data)
│   ├── YYYY-MM-DD.md     # 按日归档的流水账
│   └── ...
├── knowledge/            # [System Output] 归纳后的知识库 (Digested Data)
│   ├── [topic].md        # 特定话题的结构化知识
│   └── ...
```

### 1.2 Markdown 格式规范 (Raw Logs)
每日日志文件 (`logs/YYYY-MM-DD.md`) 采用追加模式，通过二级标题区分条目。
```markdown
---
date: 2026-01-07
tags: [cumulative, tags, list]
---

## topic_name (10:00:00)
... 记忆正文 ...

## another_topic (11:30:00)
... 更多内容 ...
```

## 2. 核心组件 (Core Components)

### 2.1 MemoryStore (`src/store.py`)
- **Git Wrapper**: 负责 `git init`, `add`, `commit`, `log` 等操作。
- **Metadata Manager**: 负责 Front Matter 的解析与合并。
- **Daily Logger**: 负责将所有 `remember` 请求追加到当日的日志文件中，并更新文件头的累积标签。

### 2.2 Digest Worker (`src/digest/`) - *Planned*
一个独立的后台服务，负责将 `logs/` 中的原始数据归纳为 `knowledge/` 中的结构化知识。
- **架构**: "GitOps for Memory"。
- **状态管理**: 无状态设计。通过 Git Commit Message 中的 `Ref-Commit: <hash>` 标记来追踪处理进度。
- **流程**: 
    1. `git log` 查找上次机器处理的 Commit Hash。
    2. `git diff` 找出自上次处理以来 `logs/` 目录的新增内容。
    3. 调用 LLM 进行语义归纳与合并。
    4. 提交更新到 `knowledge/`，并在 Commit Message 中标记当前的 HEAD Hash。
- **详见**: `src/digest/README.md`

### 2.3 MCP Server (`src/server.py`)
利用 FastMCP 暴露以下工具：
- `remember(content, topic, tags)`: 将记忆追加到 `logs/YYYY-MM-DD.md`。
- `recall(query, topic)`: (短期) 扫描所有日志文件；(长期) 优先查询 `knowledge/`，辅以 `logs/` 的最新增量。
- `history/graph`: *待重构，将基于 knowledge 目录实现。*

## 3. 演进逻辑
- **摄入策略**: 极简追加。用户无需关心文件名，所有上下文自动落盘到当日日志。
- **消化策略**: 异步增量。后台 Worker 负责“整理房间”，将散乱的日志整理为有序的知识。

## context/project/pyproject.toml (16:39:23)
[project]
name = "gitmem"
version = "0.1.0"
description = "Git-backed, Metadata-enriched Memory Service for MCP"
requires-python = ">=3.10"
dependencies = [
    "mcp[cli]>=0.1.0",
    "gitpython>=3.1.0",
    "python-frontmatter>=1.0.0",
    "pyyaml>=6.0",
    "fastapi>=0.100.0",
    "uvicorn>=0.20.0",
    "httpx>=0.24.0",
    "openai>=1.0.0",
    "python-dotenv>=1.0.0",
    "pydantic>=2.0.0"
]

[project.scripts]
gitmem = "src.server:main"

## context/user/GEMINI.md (16:39:27)
## Gemini Added Memories
- For QuantServer project, always follow these core principles: 1) NO hidden defaults for quant variables (must be explicit/from config); 2) Transparent error reporting (no masking errors with fake 100% lines); 3) Strict data integrity validation before calculation; 4) Absolute logic parity between backtest and production modules.
- MANDATORY: All UI/Frontend changes in QuantServer must be verified locally before pushing to Render. Workflow: 1) Run local server via run_server.py; 2) Use browser tools to visit http://localhost:8000/view/QQQ/1y; 3) Check console for Syntax/Type errors; 4) Verify chart rendering and interactions. Do NOT push if local verification fails.

## context/project/git_diff (16:39:30)
diff --git a/backend.log b/backend.log
index 423bd51..cd48332 100644
--- a/backend.log
+++ b/backend.log
@@ -1,5 +1,5 @@
-INFO:     Started server process [8580]
+INFO:     Started server process [35868]
 INFO:     Waiting for application startup.
-2026-01-07 21:50:19,300 - gitmem-backend - INFO - Digest consumer started.
+2026-01-08 16:33:37,751 - gitmem-backend - INFO - Digest consumer started.
 INFO:     Application startup complete.
 INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
diff --git a/src/backend.py b/src/backend.py
index 2819525..b592aa7 100644
--- a/src/backend.py
+++ b/src/backend.py
@@ -2,25 +2,25 @@ import os
 import sys
 import asyncio
 import logging
+import tempfile
+import hashlib
+import shutil
 from contextlib import asynccontextmanager
-from fastapi import FastAPI, HTTPException, BackgroundTasks
+from fastapi import FastAPI, HTTPException
 from pydantic import BaseModel
+from git import Repo
 
 # Add project root to sys.path
 project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
 if project_root not in sys.path:
     sys.path.insert(0, project_root)
 
-from src.store import MemoryStore
 from src.digest.worker import DigestWorker
 
 # Setup Logging
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger("gitmem-backend")
 
-# Initialize Store (for repo path calculation)
-store = MemoryStore()
-
 # Global Queue
 digest_queue = asyncio.Queue()
 
@@ -29,6 +29,56 @@ class TriggerRequest(BaseModel):
     git_url: str
     git_token: str | None = None
 
+# --- Helper ---
+def _prepare_backend_repo(git_url: str, git_token: str | None) -> str:
+    """
+    Prepare a temporary clone of the repo in the system temp directory.
+    This ensures isolation from any local user config or other services.
+    """
+    # 1. Calculate Temp Path
+    url_hash = hashlib.sha256(git_url.encode()).hexdigest()[:12]
+    base_tmp = os.path.join(tempfile.gettempdir(), "gitmem-backend")
+    repo_path = os.path.join(base_tmp, url_hash)
+    
+    os.makedirs(base_tmp, exist_ok=True)
+    
+    # 2. Authenticate URL
+    auth_url = git_url
+    if git_token and git_url.startswith("https://"):
+        auth_url = git_url.replace("https://", f"https://x-access-token:{git_token}@")
+
+    # 3. Clone or Update
+    if not os.path.exists(repo_path):
+        logger.info(f"Cloning to temp: {repo_path}")
+        try:
+            Repo.clone_from(auth_url, repo_path)
+        except Exception as e:
+            logger.error(f"Clone failed: {e}")
+            raise e
+    else:
+        logger.info(f"Updating temp repo: {repo_path}")
+        try:
+            repo = Repo(repo_path)
+            # Update remote in case token changed
+            if 'origin' in repo.remotes:
+                repo.remotes.origin.set_url(auth_url)
+            repo.remotes.origin.pull()
+        except Exception as e:
+            logger.error(f"Pull failed: {e}")
+            # If repo is corrupt, nuking it is a valid backend strategy
+            logger.warning("Nuking corrupt repo and re-cloning...")
+            shutil.rmtree(repo_path)
+            Repo.clone_from(auth_url, repo_path)
+            
+    # Configure Bot Identity for this temp repo
+    repo = Repo(repo_path)
+    with repo.config_writer() as git_config:
+        if not git_config.has_option('user', 'name'):
+            git_config.set_value('user', 'name', "GitMem Backend")
+            git_config.set_value('user', 'email', "backend@gitmem.cloud")
+
+    return repo_path
+
 # --- Background Consumer ---
 async def digest_consumer():
     """Consume tasks from the queue and run the worker."""
@@ -36,14 +86,21 @@ async def digest_consumer():
     while True:
         try:
             task = await digest_queue.get()
-            repo_path = task.get("repo_path")
+            git_url = task.get("git_url")
+            git_token = task.get("git_token")
             
-            if repo_path:
-                logger.info(f"Starting digest for: {repo_path}")
-                # Blocking IO runs in thread pool
-                worker = DigestWorker(repo_path)
-                await asyncio.to_thread(worker.run)
-                logger.info(f"Finished digest for: {repo_path}")
+            if git_url:
+                logger.info(f"Processing digest for: {git_url}")
+                try:
+                    # Prepare repo in thread to avoid blocking loop
+                    repo_path = await asyncio.to_thread(_prepare_backend_repo, git_url, git_token)
+                    
+                    # Run Worker
+                    worker = DigestWorker(repo_path)
+                    await asyncio.to_thread(worker.run)
+                    logger.info(f"Finished digest for: {git_url}")
+                except Exception as e:
+                    logger.error(f"Digest failed for {git_url}: {e}")
             
             digest_queue.task_done()
         except asyncio.CancelledError:
@@ -51,7 +108,7 @@ async def digest_consumer():
         except Exception as e:
             logger.error(f"Error in consumer: {e}")
             # Avoid queue lockup
-            if task: 
+            if 'task' in locals() and task: 
                 digest_queue.task_done()
 
 # --- Lifecycle ---
@@ -74,13 +131,12 @@ app = FastAPI(title="GitMem Backend", lifespan=lifespan)
 async def trigger_digest(request: TriggerRequest):
     """Trigger a background digest for the given repo."""
     try:
-        # Calculate local path using Store logic
-        repo_path = store._get_user_repo_path(request.git_url)
-        
-        # Enqueue
-        await digest_queue.put({"repo_path": repo_path})
-        
-        return {"status": "queued", "repo_path": repo_path}
+        # Just enqueue parameters. Let the worker handle path and cloning.
+        await digest_queue.put({
+            "git_url": request.git_url, 
+            "git_token": request.git_token
+        })
+        return {"status": "queued", "url": request.git_url}
     except Exception as e:
         logger.error(f"Trigger failed: {e}")
         raise HTTPException(status_code=500, detail=str(e))
diff --git a/src/digest/__pycache__/worker.cpython-313.pyc b/src/digest/__pycache__/worker.cpython-313.pyc
index 3558a4a..6fec236 100644
Binary files a/src/digest/__pycache__/worker.cpython-313.pyc and b/src/digest/__pycache__/worker.cpython-313.pyc differ
diff --git a/src/digest/worker.py b/src/digest/worker.py
index 2f5fc74..1f117cf 100644
--- a/src/digest/worker.py
+++ b/src/digest/worker.py
@@ -192,6 +192,14 @@ class GitClient:
         self.repo.index.commit(f"{message}\n\nRef-Commit: {ref_hash}")
         logger.info(f"Committed digest work. Ref: {ref_hash}")
 
+    def push(self):
+        try:
+            origin = self.repo.remote(name='origin')
+            origin.push()
+            logger.info("Pushed changes to remote.")
+        except Exception as e:
+            logger.error(f"Push failed: {e}")
+
 class DigestWorker:
     def __init__(self, repo_path: str):
         self.repo_path = repo_path
@@ -212,14 +220,17 @@ class DigestWorker:
         
         last_hash = self.git.get_last_processed_hash()
         current_head = self.git.repo.head.commit.hexsha
+        logger.info(f"Last Hash: {last_hash}, Current Head: {current_head}")
         
         if last_hash == current_head:
             logger.info("Up to date.")
             return
 
         commits = self.git.get_pending_commits(last_hash)
+        logger.info(f"Found {len(commits)} pending commits.")
         if not commits:
             self.git.commit_work(current_head, "Digest: fast-forward")
+            self.git.push()
             return
         
         # 1. Extract Raw Entries
@@ -228,6 +239,7 @@ class DigestWorker:
         
         if not entries:
              self.git.commit_work(current_head, "Digest: no parseable content")
+             self.git.push()
              return
 
         # 2. Group by Canonical Topic
@@ -259,8 +271,9 @@ class DigestWorker:
             with open(topic_file, 'w') as f:
                 f.write(final_text)
                 
-        # 4. Commit
+        # 4. Commit & Push
         self.git.commit_work(current_head)
+        self.git.push()
 
 import argparse
 import sys
@@ -309,15 +322,6 @@ if __name__ == "__main__":
         # 2. Run Digest
         worker = DigestWorker(repo_path)
         worker.run()
-        
-        # 3. Push changes back
-        if repo.is_dirty() or repo.index.diff(repo.head.commit):
-             # Just in case worker.run() didn't push (it currently only commits)
-             # But our commit_work commits to index.
-             # Ideally worker should push. Let's add push here for safety.
-             origin = repo.remote(name='origin')
-             origin.push()
-             logger.info("Pushed changes to remote.")
              
     except Exception as e:
         logger.error(f"Worker failed: {e}")
diff --git a/src/server.py b/src/server.py
index d627972..d3166bf 100644
--- a/src/server.py
+++ b/src/server.py
@@ -10,6 +10,7 @@ if project_root not in sys.path:
 from mcp.server.fastmcp import FastMCP
 from src.store import MemoryStore
 from typing import List, Optional
+from datetime import datetime
 
 # Setup error logging for debugging startup
 LOG_FILE = os.path.expanduser("~/.gitmem/error.log")
@@ -42,6 +43,20 @@ CONTEXT_SOURCES = {
     ]
 }
 
+def _trigger_digest(git_url: str, git_token: Optional[str] = None):
+    """Notify the backend that new content is available for digestion."""
+    backend_url = os.environ.get("GITMEM_BACKEND_URL", "http://localhost:8000")
+    try:
+        with httpx.Client(timeout=1.0) as client:
+            payload = {"git_url": git_url, "git_token": git_token}
+            resp = client.post(f"{backend_url}/trigger-digest", json=payload)
+            if resp.status_code != 200:
+                with open(os.path.expanduser("~/.gitmem/trigger_error.log"), "a") as f:
+                    f.write(f"[{datetime.now()}] Backend returned {resp.status_code}: {resp.text}\n")
+    except Exception as e:
+        with open(os.path.expanduser("~/.gitmem/trigger_error.log"), "a") as f:
+            f.write(f"[{datetime.now()}] Trigger failed: {str(e)}\n")
+
 @mcp.tool()
 def capture_context(scope: str = "all", git_url: str = None, git_token: str = None) -> str:
     """
@@ -55,6 +70,13 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
     logs = []
     scopes_to_process = []
     
+    # Resolve config
+    current_git_url = git_url or os.environ.get("GITMEM_URL")
+    current_git_token = git_token or os.environ.get("GITMEM_TOKEN")
+
+    if not current_git_url:
+        return "Error: GITMEM_URL is missing."
+
     if scope == "all":
         scopes_to_process = ["project", "user", "system"]
     elif scope in CONTEXT_SOURCES:
@@ -66,7 +88,6 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
     for s in scopes_to_process:
         patterns = CONTEXT_SOURCES.get(s, [])
         for pattern in patterns:
-            # Handle user expansion and relative paths
             full_path = os.path.expanduser(pattern)
             if not os.path.isabs(full_path):
                 full_path = os.path.abspath(full_path)
@@ -75,9 +96,6 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
                 try:
                     with open(full_path, 'r', encoding='utf-8') as f:
                         content = f.read()
-                    
-                    # Store it
-                    # Topic convention: context/{scope}/{filename}
                     filename = os.path.basename(full_path)
                     topic = f"context/{s}/{filename}"
                     
@@ -85,17 +103,16 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
                         content=content,
                         topic=topic,
                         tags=["context", s, "auto-capture"],
-                        git_url=git_url,
-                        git_token=git_token
+                        git_url=current_git_url,
+                        git_token=current_git_token
                     )
                     logs.append(f"[File] {filename}: {res}")
                 except Exception as e:
                     logs.append(f"[Error] Failed to read {full_path}: {e}")
 
-    # 2. Capture Git Diff (Only for project scope or all)
+    # 2. Capture Git Diff
     if "project" in scopes_to_process:
         try:
-            # Check for unstaged changes
             diff_proc = subprocess.run(["git", "diff", "HEAD"], capture_output=True, text=True)
             if diff_proc.returncode == 0 and diff_proc.stdout.strip():
                 diff_content = diff_proc.stdout
@@ -103,14 +120,15 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
                     content=diff_content,
                     topic="context/project/git_diff",
                     tags=["context", "project", "diff"],
-                    git_url=git_url,
-                    git_token=git_token
+                    git_url=current_git_url,
+                    git_token=current_git_token
                 )
                 logs.append(f"[Diff] Git Diff captured: {res}")
-            else:
-                logs.append("[Diff] No active changes found in git.")
         except Exception as e:
-            logs.append(f"[Diff] Error capturing git diff: {e}")
+            logs.append(f"[Diff] Error: {e}")
+
+    # 3. Trigger Digest once for all updates
+    _trigger_digest(current_git_url, current_git_token)
 
     return "\n".join(logs)
 
@@ -119,36 +137,17 @@ def remember(content: str, topic: str = "global", tags: List[str] = None,
              dependencies: List[str] = None, git_url: str = None, git_token: str = None) -> str:
     """
     Store or update knowledge in a specific Git repository.
-    
-    Args:
-        content: The text content to remember.
-        topic: The general topic (e.g., 'project_alpha'). Defaults to 'global'.
-        tags: Optional list of tags.
-        dependencies: Optional list of related topics.
-        git_url: The HTTPS URL of your git repository (required for multi-user).
-        git_token: Personal Access Token for authentication.
     """
-    result = store.remember(content, topic, tags, dependencies, git_url, git_token)
+    current_git_url = git_url or os.environ.get("GITMEM_URL")
+    current_git_token = git_token or os.environ.get("GITMEM_TOKEN")
+
+    if not current_git_url:
+        return "Error: GITMEM_URL is missing."
+
+    result = store.remember(content, topic, tags, dependencies, current_git_url, current_git_token)
     
-    # Trigger Remote Backend Digestion (Hybrid Architecture)
-    # Only if storage was successful and we have a backend URL
-    if "Error:" not in result and git_url:
-        backend_url = os.environ.get("GITMEM_BACKEND_URL", "http://localhost:8000")
-        try:
-            # We use a short timeout because we don't want to block the CLI user
-            # waiting for the backend to acknowledge.
-            # Ideally this should be a background task, but in FastMCP sync tool context,
-            # a quick HTTP call is acceptable.
-            with httpx.Client(timeout=1.0) as client:
-                payload = {"git_url": git_url, "git_token": git_token}
-                # Fire and forget-ish (we wait for response but don't parse it deeply)
-                resp = client.post(f"{backend_url}/trigger-digest", json=payload)
-                if resp.status_code == 200:
-                    pass # Success
-        except Exception:
-            # Silently fail network trigger to avoid annoying the user.
-            # The memory is safely stored in Git anyway.
-            pass
+    if "Error:" not in result:
+        _trigger_digest(current_git_url, current_git_token)
 
     return result
 
diff --git a/sse_server.log b/sse_server.log
deleted file mode 100644
index d45b554..0000000
--- a/sse_server.log
+++ /dev/null
@@ -1,4 +0,0 @@
-Traceback (most recent call last):
-  File "/Users/zhoufan/Public/workspace/gitmem/src/sse_app.py", line 12, in <module>
-    from src.store import MemoryStore
-ModuleNotFoundError: No module named 'src'

## context/project/GEMINI.md (16:42:28)
# GitMem Project Context

## Vision
GitMem 是一个利用 Git 作为后端存储的语义化记忆服务。它通过 MCP (Model Context Protocol) 协议为 AI Agent 提供持久化、可回溯、具备“摄入-消化”闭环能力的长期记忆体。

## Core Principles
1. **摄入重于分类**: 原始记忆以“流水账”形式按日归档（`logs/YYYY-MM-DD.md`），保证上下文完整性。
2. **GitOps 驱动**: 利用 Git Commit 作为事件总线，实现无状态的后台处理。
3. **混合架构 (Hybrid)**: 本地 MCP Server (FastMCP) 负责极速读写，远程 Backend (FastAPI) 负责异步 LLM 归纳。
4. **双层存储**: `logs/` 存放原始数据，`knowledge/` 存放结构化知识。

## Current Status
- [x] **混合架构落地**: 
    - `src/server.py`: 本地客户端，支持 Git 缓存读写 + HTTP Trigger。
    - `src/backend.py`: 远程服务端，FastAPI + Async Queue，支持多租户归纳。
- [x] **存储引擎重构**: 
    - 原始记忆存入 `logs/` 子目录。
    - 每日日志头部自动维护累积 Tags。
- [x] **智能归纳服务 (Digest Worker)**:
    - 实现了基于 Git Diff 的增量解析。
    - 实现了 LLM 驱动的话题路由 (`index.yaml`) 和全量重写归纳。
- [x] **多租户支持**: 通过 Trigger 动态拉取用户 Repo，服务无状态化。

## Key Decisions
- **通信协议**: 本地与后台之间采用 RESTful (`POST /trigger-digest`) 而非 SSE，简化架构。
- **状态追踪**: 严格使用 Git Commit Message 中的 `Ref-Commit` 标记，拒绝本地状态文件。
- **归纳策略**: 采用“全量重写 (Full Rewrite)”模式更新知识库文件，确保文档结构的一致性。

## TODOs
- [ ] **语义化检索重构**:
    - **Smart Recall**: 优先检索 `knowledge/` 中的结构化数据，辅以 `logs/` 的最新增量。
    - **History**: 基于 `knowledge` 文件的版本历史生成话题演进图。
    - **Graph**: 生成基于 `index.yaml` 和引用关系的知识热力图。
- [ ] **高级归纳能力**:
    - 支持冲突检测与人工介入标记。
    - 定期全量 Compaction（合并碎片化的小话题）。

## context/project/README.md (16:42:30)
# GitMem

GitMem is a semantic memory service for AI Agents, backed by Git. It treats memory as a "Living Codebase".

## Architecture

GitMem employs a **Hybrid Architecture** to balance latency and intelligence:

1.  **Local MCP Server (`src/server.py`)**: 
    - Runs locally (Stdio) with your Agent (Claude, Windsurf, Gemini).
    - Provides instant `remember` and `recall` capabilities using a local Git cache.
    - Triggers the backend service upon new writes.

2.  **Backend Service (`src/backend.py`)**:
    - Runs permanently (e.g., on Render).
    - Receives `trigger-digest` signals.
    - Runs an asynchronous **Digest Worker** that uses LLMs to summarize raw logs into structured knowledge.

3.  **Storage (`Git Repo`)**:
    - `logs/`: Raw, append-only daily logs (High Fidelity).
    - `knowledge/`: AI-curated, structured markdown files (High Utility).

## Setup

### 1. Requirements
- Python 3.10+
- OpenAI API Key (for digestion)
- A Git Repository (GitHub/GitLab)

### 2. Installation
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 3. Running the Backend
Create `src/digest/.env` with your LLM keys (see `.env.example`).
```bash
python src/backend.py
```

### 4. Configuring Your Agent
Add the local server to your `claude_desktop_config.json` or equivalent:
```json
"gitmem": {
  "command": "python",
  "args": ["/path/to/gitmem/src/server.py"],
  "env": {
    "GITMEM_URL": "https://github.com/your/memory-repo.git",
    "GITMEM_TOKEN": "your-git-token",
    "GITMEM_BACKEND_URL": "http://localhost:8000"
  }
}
```

## Usage
Simply tell your Agent:
> "Remember that the deployment port for QuantServer is 9090."

GitMem will:
1.  Save it to `logs/YYYY-MM-DD.md`.
2.  Trigger the backend.
3.  Update `knowledge/quant_server.md` automatically.

## context/project/DESIGN.md (16:42:33)
# GitMem Technical Design

## 1. 存储架构 (Storage Schema)

### 1.1 目录结构
我们采用 **"双层存储架构"**，区分原始摄入与加工知识。
```text
storage/
├── logs/                 # [User Input] 原始记忆 (Raw Data)
│   ├── YYYY-MM-DD.md     # 按日归档的流水账
│   └── ...
├── knowledge/            # [System Output] 归纳后的知识库 (Digested Data)
│   ├── [topic].md        # 特定话题的结构化知识
│   └── ...
```

### 1.2 Markdown 格式规范 (Raw Logs)
每日日志文件 (`logs/YYYY-MM-DD.md`) 采用追加模式，通过二级标题区分条目。
```markdown
---
date: 2026-01-07
tags: [cumulative, tags, list]
---

## topic_name (10:00:00)
... 记忆正文 ...

## another_topic (11:30:00)
... 更多内容 ...
```

## 2. 核心组件 (Core Components)

### 2.1 MemoryStore (`src/store.py`)
- **Git Wrapper**: 负责 `git init`, `add`, `commit`, `log` 等操作。
- **Metadata Manager**: 负责 Front Matter 的解析与合并。
- **Daily Logger**: 负责将所有 `remember` 请求追加到当日的日志文件中，并更新文件头的累积标签。

### 2.2 Digest Worker (`src/digest/`) - *Planned*
一个独立的后台服务，负责将 `logs/` 中的原始数据归纳为 `knowledge/` 中的结构化知识。
- **架构**: "GitOps for Memory"。
- **状态管理**: 无状态设计。通过 Git Commit Message 中的 `Ref-Commit: <hash>` 标记来追踪处理进度。
- **流程**: 
    1. `git log` 查找上次机器处理的 Commit Hash。
    2. `git diff` 找出自上次处理以来 `logs/` 目录的新增内容。
    3. 调用 LLM 进行语义归纳与合并。
    4. 提交更新到 `knowledge/`，并在 Commit Message 中标记当前的 HEAD Hash。
- **详见**: `src/digest/README.md`

### 2.3 MCP Server (`src/server.py`)
利用 FastMCP 暴露以下工具：
- `remember(content, topic, tags)`: 将记忆追加到 `logs/YYYY-MM-DD.md`。
- `recall(query, topic)`: (短期) 扫描所有日志文件；(长期) 优先查询 `knowledge/`，辅以 `logs/` 的最新增量。
- `history/graph`: *待重构，将基于 knowledge 目录实现。*

## 3. 演进逻辑
- **摄入策略**: 极简追加。用户无需关心文件名，所有上下文自动落盘到当日日志。
- **消化策略**: 异步增量。后台 Worker 负责“整理房间”，将散乱的日志整理为有序的知识。

## context/project/pyproject.toml (16:42:36)
[project]
name = "gitmem"
version = "0.1.0"
description = "Git-backed, Metadata-enriched Memory Service for MCP"
requires-python = ">=3.10"
dependencies = [
    "mcp[cli]>=0.1.0",
    "gitpython>=3.1.0",
    "python-frontmatter>=1.0.0",
    "pyyaml>=6.0",
    "fastapi>=0.100.0",
    "uvicorn>=0.20.0",
    "httpx>=0.24.0",
    "openai>=1.0.0",
    "python-dotenv>=1.0.0",
    "pydantic>=2.0.0"
]

[project.scripts]
gitmem = "src.server:main"

## context/user/GEMINI.md (16:42:39)
## Gemini Added Memories
- For QuantServer project, always follow these core principles: 1) NO hidden defaults for quant variables (must be explicit/from config); 2) Transparent error reporting (no masking errors with fake 100% lines); 3) Strict data integrity validation before calculation; 4) Absolute logic parity between backtest and production modules.
- MANDATORY: All UI/Frontend changes in QuantServer must be verified locally before pushing to Render. Workflow: 1) Run local server via run_server.py; 2) Use browser tools to visit http://localhost:8000/view/QQQ/1y; 3) Check console for Syntax/Type errors; 4) Verify chart rendering and interactions. Do NOT push if local verification fails.

## context/project/git_diff (16:42:41)
diff --git a/backend.log b/backend.log
index 423bd51..cd48332 100644
--- a/backend.log
+++ b/backend.log
@@ -1,5 +1,5 @@
-INFO:     Started server process [8580]
+INFO:     Started server process [35868]
 INFO:     Waiting for application startup.
-2026-01-07 21:50:19,300 - gitmem-backend - INFO - Digest consumer started.
+2026-01-08 16:33:37,751 - gitmem-backend - INFO - Digest consumer started.
 INFO:     Application startup complete.
 INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
diff --git a/src/backend.py b/src/backend.py
index 2819525..b592aa7 100644
--- a/src/backend.py
+++ b/src/backend.py
@@ -2,25 +2,25 @@ import os
 import sys
 import asyncio
 import logging
+import tempfile
+import hashlib
+import shutil
 from contextlib import asynccontextmanager
-from fastapi import FastAPI, HTTPException, BackgroundTasks
+from fastapi import FastAPI, HTTPException
 from pydantic import BaseModel
+from git import Repo
 
 # Add project root to sys.path
 project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
 if project_root not in sys.path:
     sys.path.insert(0, project_root)
 
-from src.store import MemoryStore
 from src.digest.worker import DigestWorker
 
 # Setup Logging
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger("gitmem-backend")
 
-# Initialize Store (for repo path calculation)
-store = MemoryStore()
-
 # Global Queue
 digest_queue = asyncio.Queue()
 
@@ -29,6 +29,56 @@ class TriggerRequest(BaseModel):
     git_url: str
     git_token: str | None = None
 
+# --- Helper ---
+def _prepare_backend_repo(git_url: str, git_token: str | None) -> str:
+    """
+    Prepare a temporary clone of the repo in the system temp directory.
+    This ensures isolation from any local user config or other services.
+    """
+    # 1. Calculate Temp Path
+    url_hash = hashlib.sha256(git_url.encode()).hexdigest()[:12]
+    base_tmp = os.path.join(tempfile.gettempdir(), "gitmem-backend")
+    repo_path = os.path.join(base_tmp, url_hash)
+    
+    os.makedirs(base_tmp, exist_ok=True)
+    
+    # 2. Authenticate URL
+    auth_url = git_url
+    if git_token and git_url.startswith("https://"):
+        auth_url = git_url.replace("https://", f"https://x-access-token:{git_token}@")
+
+    # 3. Clone or Update
+    if not os.path.exists(repo_path):
+        logger.info(f"Cloning to temp: {repo_path}")
+        try:
+            Repo.clone_from(auth_url, repo_path)
+        except Exception as e:
+            logger.error(f"Clone failed: {e}")
+            raise e
+    else:
+        logger.info(f"Updating temp repo: {repo_path}")
+        try:
+            repo = Repo(repo_path)
+            # Update remote in case token changed
+            if 'origin' in repo.remotes:
+                repo.remotes.origin.set_url(auth_url)
+            repo.remotes.origin.pull()
+        except Exception as e:
+            logger.error(f"Pull failed: {e}")
+            # If repo is corrupt, nuking it is a valid backend strategy
+            logger.warning("Nuking corrupt repo and re-cloning...")
+            shutil.rmtree(repo_path)
+            Repo.clone_from(auth_url, repo_path)
+            
+    # Configure Bot Identity for this temp repo
+    repo = Repo(repo_path)
+    with repo.config_writer() as git_config:
+        if not git_config.has_option('user', 'name'):
+            git_config.set_value('user', 'name', "GitMem Backend")
+            git_config.set_value('user', 'email', "backend@gitmem.cloud")
+
+    return repo_path
+
 # --- Background Consumer ---
 async def digest_consumer():
     """Consume tasks from the queue and run the worker."""
@@ -36,14 +86,21 @@ async def digest_consumer():
     while True:
         try:
             task = await digest_queue.get()
-            repo_path = task.get("repo_path")
+            git_url = task.get("git_url")
+            git_token = task.get("git_token")
             
-            if repo_path:
-                logger.info(f"Starting digest for: {repo_path}")
-                # Blocking IO runs in thread pool
-                worker = DigestWorker(repo_path)
-                await asyncio.to_thread(worker.run)
-                logger.info(f"Finished digest for: {repo_path}")
+            if git_url:
+                logger.info(f"Processing digest for: {git_url}")
+                try:
+                    # Prepare repo in thread to avoid blocking loop
+                    repo_path = await asyncio.to_thread(_prepare_backend_repo, git_url, git_token)
+                    
+                    # Run Worker
+                    worker = DigestWorker(repo_path)
+                    await asyncio.to_thread(worker.run)
+                    logger.info(f"Finished digest for: {git_url}")
+                except Exception as e:
+                    logger.error(f"Digest failed for {git_url}: {e}")
             
             digest_queue.task_done()
         except asyncio.CancelledError:
@@ -51,7 +108,7 @@ async def digest_consumer():
         except Exception as e:
             logger.error(f"Error in consumer: {e}")
             # Avoid queue lockup
-            if task: 
+            if 'task' in locals() and task: 
                 digest_queue.task_done()
 
 # --- Lifecycle ---
@@ -74,13 +131,12 @@ app = FastAPI(title="GitMem Backend", lifespan=lifespan)
 async def trigger_digest(request: TriggerRequest):
     """Trigger a background digest for the given repo."""
     try:
-        # Calculate local path using Store logic
-        repo_path = store._get_user_repo_path(request.git_url)
-        
-        # Enqueue
-        await digest_queue.put({"repo_path": repo_path})
-        
-        return {"status": "queued", "repo_path": repo_path}
+        # Just enqueue parameters. Let the worker handle path and cloning.
+        await digest_queue.put({
+            "git_url": request.git_url, 
+            "git_token": request.git_token
+        })
+        return {"status": "queued", "url": request.git_url}
     except Exception as e:
         logger.error(f"Trigger failed: {e}")
         raise HTTPException(status_code=500, detail=str(e))
diff --git a/src/digest/__pycache__/worker.cpython-313.pyc b/src/digest/__pycache__/worker.cpython-313.pyc
index 3558a4a..6fec236 100644
Binary files a/src/digest/__pycache__/worker.cpython-313.pyc and b/src/digest/__pycache__/worker.cpython-313.pyc differ
diff --git a/src/digest/worker.py b/src/digest/worker.py
index 2f5fc74..1f117cf 100644
--- a/src/digest/worker.py
+++ b/src/digest/worker.py
@@ -192,6 +192,14 @@ class GitClient:
         self.repo.index.commit(f"{message}\n\nRef-Commit: {ref_hash}")
         logger.info(f"Committed digest work. Ref: {ref_hash}")
 
+    def push(self):
+        try:
+            origin = self.repo.remote(name='origin')
+            origin.push()
+            logger.info("Pushed changes to remote.")
+        except Exception as e:
+            logger.error(f"Push failed: {e}")
+
 class DigestWorker:
     def __init__(self, repo_path: str):
         self.repo_path = repo_path
@@ -212,14 +220,17 @@ class DigestWorker:
         
         last_hash = self.git.get_last_processed_hash()
         current_head = self.git.repo.head.commit.hexsha
+        logger.info(f"Last Hash: {last_hash}, Current Head: {current_head}")
         
         if last_hash == current_head:
             logger.info("Up to date.")
             return
 
         commits = self.git.get_pending_commits(last_hash)
+        logger.info(f"Found {len(commits)} pending commits.")
         if not commits:
             self.git.commit_work(current_head, "Digest: fast-forward")
+            self.git.push()
             return
         
         # 1. Extract Raw Entries
@@ -228,6 +239,7 @@ class DigestWorker:
         
         if not entries:
              self.git.commit_work(current_head, "Digest: no parseable content")
+             self.git.push()
              return
 
         # 2. Group by Canonical Topic
@@ -259,8 +271,9 @@ class DigestWorker:
             with open(topic_file, 'w') as f:
                 f.write(final_text)
                 
-        # 4. Commit
+        # 4. Commit & Push
         self.git.commit_work(current_head)
+        self.git.push()
 
 import argparse
 import sys
@@ -309,15 +322,6 @@ if __name__ == "__main__":
         # 2. Run Digest
         worker = DigestWorker(repo_path)
         worker.run()
-        
-        # 3. Push changes back
-        if repo.is_dirty() or repo.index.diff(repo.head.commit):
-             # Just in case worker.run() didn't push (it currently only commits)
-             # But our commit_work commits to index.
-             # Ideally worker should push. Let's add push here for safety.
-             origin = repo.remote(name='origin')
-             origin.push()
-             logger.info("Pushed changes to remote.")
              
     except Exception as e:
         logger.error(f"Worker failed: {e}")
diff --git a/src/server.py b/src/server.py
index d627972..d3166bf 100644
--- a/src/server.py
+++ b/src/server.py
@@ -10,6 +10,7 @@ if project_root not in sys.path:
 from mcp.server.fastmcp import FastMCP
 from src.store import MemoryStore
 from typing import List, Optional
+from datetime import datetime
 
 # Setup error logging for debugging startup
 LOG_FILE = os.path.expanduser("~/.gitmem/error.log")
@@ -42,6 +43,20 @@ CONTEXT_SOURCES = {
     ]
 }
 
+def _trigger_digest(git_url: str, git_token: Optional[str] = None):
+    """Notify the backend that new content is available for digestion."""
+    backend_url = os.environ.get("GITMEM_BACKEND_URL", "http://localhost:8000")
+    try:
+        with httpx.Client(timeout=1.0) as client:
+            payload = {"git_url": git_url, "git_token": git_token}
+            resp = client.post(f"{backend_url}/trigger-digest", json=payload)
+            if resp.status_code != 200:
+                with open(os.path.expanduser("~/.gitmem/trigger_error.log"), "a") as f:
+                    f.write(f"[{datetime.now()}] Backend returned {resp.status_code}: {resp.text}\n")
+    except Exception as e:
+        with open(os.path.expanduser("~/.gitmem/trigger_error.log"), "a") as f:
+            f.write(f"[{datetime.now()}] Trigger failed: {str(e)}\n")
+
 @mcp.tool()
 def capture_context(scope: str = "all", git_url: str = None, git_token: str = None) -> str:
     """
@@ -55,6 +70,13 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
     logs = []
     scopes_to_process = []
     
+    # Resolve config
+    current_git_url = git_url or os.environ.get("GITMEM_URL")
+    current_git_token = git_token or os.environ.get("GITMEM_TOKEN")
+
+    if not current_git_url:
+        return "Error: GITMEM_URL is missing."
+
     if scope == "all":
         scopes_to_process = ["project", "user", "system"]
     elif scope in CONTEXT_SOURCES:
@@ -66,7 +88,6 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
     for s in scopes_to_process:
         patterns = CONTEXT_SOURCES.get(s, [])
         for pattern in patterns:
-            # Handle user expansion and relative paths
             full_path = os.path.expanduser(pattern)
             if not os.path.isabs(full_path):
                 full_path = os.path.abspath(full_path)
@@ -75,9 +96,6 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
                 try:
                     with open(full_path, 'r', encoding='utf-8') as f:
                         content = f.read()
-                    
-                    # Store it
-                    # Topic convention: context/{scope}/{filename}
                     filename = os.path.basename(full_path)
                     topic = f"context/{s}/{filename}"
                     
@@ -85,17 +103,16 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
                         content=content,
                         topic=topic,
                         tags=["context", s, "auto-capture"],
-                        git_url=git_url,
-                        git_token=git_token
+                        git_url=current_git_url,
+                        git_token=current_git_token
                     )
                     logs.append(f"[File] {filename}: {res}")
                 except Exception as e:
                     logs.append(f"[Error] Failed to read {full_path}: {e}")
 
-    # 2. Capture Git Diff (Only for project scope or all)
+    # 2. Capture Git Diff
     if "project" in scopes_to_process:
         try:
-            # Check for unstaged changes
             diff_proc = subprocess.run(["git", "diff", "HEAD"], capture_output=True, text=True)
             if diff_proc.returncode == 0 and diff_proc.stdout.strip():
                 diff_content = diff_proc.stdout
@@ -103,14 +120,15 @@ def capture_context(scope: str = "all", git_url: str = None, git_token: str = No
                     content=diff_content,
                     topic="context/project/git_diff",
                     tags=["context", "project", "diff"],
-                    git_url=git_url,
-                    git_token=git_token
+                    git_url=current_git_url,
+                    git_token=current_git_token
                 )
                 logs.append(f"[Diff] Git Diff captured: {res}")
-            else:
-                logs.append("[Diff] No active changes found in git.")
         except Exception as e:
-            logs.append(f"[Diff] Error capturing git diff: {e}")
+            logs.append(f"[Diff] Error: {e}")
+
+    # 3. Trigger Digest once for all updates
+    _trigger_digest(current_git_url, current_git_token)
 
     return "\n".join(logs)
 
@@ -119,36 +137,17 @@ def remember(content: str, topic: str = "global", tags: List[str] = None,
              dependencies: List[str] = None, git_url: str = None, git_token: str = None) -> str:
     """
     Store or update knowledge in a specific Git repository.
-    
-    Args:
-        content: The text content to remember.
-        topic: The general topic (e.g., 'project_alpha'). Defaults to 'global'.
-        tags: Optional list of tags.
-        dependencies: Optional list of related topics.
-        git_url: The HTTPS URL of your git repository (required for multi-user).
-        git_token: Personal Access Token for authentication.
     """
-    result = store.remember(content, topic, tags, dependencies, git_url, git_token)
+    current_git_url = git_url or os.environ.get("GITMEM_URL")
+    current_git_token = git_token or os.environ.get("GITMEM_TOKEN")
+
+    if not current_git_url:
+        return "Error: GITMEM_URL is missing."
+
+    result = store.remember(content, topic, tags, dependencies, current_git_url, current_git_token)
     
-    # Trigger Remote Backend Digestion (Hybrid Architecture)
-    # Only if storage was successful and we have a backend URL
-    if "Error:" not in result and git_url:
-        backend_url = os.environ.get("GITMEM_BACKEND_URL", "http://localhost:8000")
-        try:
-            # We use a short timeout because we don't want to block the CLI user
-            # waiting for the backend to acknowledge.
-            # Ideally this should be a background task, but in FastMCP sync tool context,
-            # a quick HTTP call is acceptable.
-            with httpx.Client(timeout=1.0) as client:
-                payload = {"git_url": git_url, "git_token": git_token}
-                # Fire and forget-ish (we wait for response but don't parse it deeply)
-                resp = client.post(f"{backend_url}/trigger-digest", json=payload)
-                if resp.status_code == 200:
-                    pass # Success
-        except Exception:
-            # Silently fail network trigger to avoid annoying the user.
-            # The memory is safely stored in Git anyway.
-            pass
+    if "Error:" not in result:
+        _trigger_digest(current_git_url, current_git_token)
 
     return result
 
diff --git a/sse_server.log b/sse_server.log
deleted file mode 100644
index d45b554..0000000
--- a/sse_server.log
+++ /dev/null
@@ -1,4 +0,0 @@
-Traceback (most recent call last):
-  File "/Users/zhoufan/Public/workspace/gitmem/src/sse_app.py", line 12, in <module>
-    from src.store import MemoryStore
-ModuleNotFoundError: No module named 'src'